{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def get_all_file_in_folder(folder):\n",
    "    for _, _, files in os.walk(folder):\n",
    "        filenames = list(files)\n",
    "        file_paths = [os.path.join(folder, f) for f in filenames]\n",
    "        return filenames, file_paths\n",
    "    return [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"../data/context/merged_tokens.csv\")\n",
    "df.drop('Unnamed: 0', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df[(df[\"score\"] == 3 )|(df[\"count\"]>1)]\n",
    "df.dropna(inplace=True)\n",
    "df = df[(df[\"score\"] == 3 )|(df[\"count\"]>10)]\n",
    "# df = df.sort_values(by=\"token\", key=lambda x: -x.str.len())\n",
    "df = df.sort_values(by=\"count\", key=lambda x: -x)\n",
    "df = df[df[\"count\"]<100000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape\n",
    "df.insert(0, 'id', range(len(df)))\n",
    "dict_df = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>token</th>\n",
       "      <th>score</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0</td>\n",
       "      <td>nam</td>\n",
       "      <td>3</td>\n",
       "      <td>80269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1</td>\n",
       "      <td>thành</td>\n",
       "      <td>3</td>\n",
       "      <td>73905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2</td>\n",
       "      <td>tiếng</td>\n",
       "      <td>3</td>\n",
       "      <td>70842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>3</td>\n",
       "      <td>diện tích</td>\n",
       "      <td>3</td>\n",
       "      <td>69009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>4</td>\n",
       "      <td>bắc</td>\n",
       "      <td>3</td>\n",
       "      <td>68083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1831834</th>\n",
       "      <td>3001541</td>\n",
       "      <td>lẫm khưu</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1831835</th>\n",
       "      <td>3001542</td>\n",
       "      <td>cao trương phản công</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1831836</th>\n",
       "      <td>3001543</td>\n",
       "      <td>lê di vua</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1831830</th>\n",
       "      <td>3001544</td>\n",
       "      <td>tcn tề quốc</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3717018</th>\n",
       "      <td>3001545</td>\n",
       "      <td>çine sağlık</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3001546 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              id                 token  score  count\n",
       "16             0                   nam      3  80269\n",
       "17             1                 thành      3  73905\n",
       "18             2                 tiếng      3  70842\n",
       "19             3             diện tích      3  69009\n",
       "20             4                   bắc      3  68083\n",
       "...          ...                   ...    ...    ...\n",
       "1831834  3001541              lẫm khưu      3      1\n",
       "1831835  3001542  cao trương phản công      3      1\n",
       "1831836  3001543             lê di vua      3      1\n",
       "1831830  3001544           tcn tề quốc      3      1\n",
       "3717018  3001545           çine sağlık      3      1\n",
       "\n",
       "[3001546 rows x 4 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df[df[\"token\"] == \"123\"].iloc[0]\n",
    "dict_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save dictionary\n",
      "\n",
      "Build sparse matrix\n",
      "Process file ../data/context/.token/wikipedia_aa.processed\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|█▏                                                                      | 1657/100000 [05:48<5:48:25,  4.70it/s]"
     ]
    }
   ],
   "source": [
    "batch = 100\n",
    "size = 1273500 #1273469\n",
    "token_size = dict_df.shape[0]\n",
    "expect_file_rows = 100000\n",
    "\n",
    "\n",
    "import scipy.sparse as sp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import pickle\n",
    "import os\n",
    "import time\n",
    "\n",
    "# save dict to file\n",
    "print(\"Save dictionary\\n\")\n",
    "# dict_df.to_csv(\"dictionary.csv\")\n",
    "\n",
    "# tokens = {}\n",
    "# for index, row in dict_df.iterrows():\n",
    "#     tokens[row['token']] = (row['id'], row['score'])\n",
    "\n",
    "# map context id to matrix id\n",
    "map_context_id_to_matrix_id = {}\n",
    "\n",
    "file_partials = [\"aa\", \"ab\", \"ac\", \"ad\", \"ae\", \"af\", \"ag\", \"ah\", \"ai\", \"aj\", \"ak\", \"al\", \"am\"]\n",
    "\n",
    "# _, filepaths = get_all_file_in_folder(\"../data/context/.token\")\n",
    "filepaths = [\n",
    "    os.path.join(\"../data/context/.token\", \"wikipedia_%s.processed\" % f)\n",
    "    for f in file_partials\n",
    "]\n",
    "# filepaths = filepaths[:2]\n",
    "\n",
    "# idx = 0\n",
    "# for fn in tqdm(filepaths):\n",
    "#     with open(fn, \"r\") as f:\n",
    "#         for l in f:\n",
    "#             jdata = json.loads(l)\n",
    "#             map_context_id_to_matrix_id[jdata[\"id\"]] = idx\n",
    "#             idx += 1\n",
    "            \n",
    "# map_matrix_id_to_context_id = {v:k for k,v in map_context_id_to_matrix_id.items()}\n",
    "\n",
    "# save map\n",
    "# with open('map_context_id_to_matrix_id.pickle', 'wb') as handle:\n",
    "#     pickle.dump(map_context_id_to_matrix_id, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "# with open('map_matrix_id_to_context_id.pickle', 'wb') as handle:\n",
    "#     pickle.dump(map_matrix_id_to_context_id, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "\n",
    "# build sparse matrix\n",
    "print(\"Build sparse matrix\")\n",
    "matrix_id = 0\n",
    "indexer = None\n",
    "\n",
    "for fn in filepaths:# tqdm(filepaths):\n",
    "    t_point = time.time()\n",
    "    b = 0\n",
    "    indptr = [0]\n",
    "    indices = []\n",
    "    data = []\n",
    "    with open(fn, \"r\") as f:\n",
    "        print(\"Process file %s\\n\" % fn)\n",
    "#         c_ = 0\n",
    "        for _ in tqdm(range(expect_file_rows)):\n",
    "            l = f.readline()\n",
    "#         for l in f:\n",
    "#             print('new')\n",
    "#             c_ += 1\n",
    "#             if c_ > 5:\n",
    "#                 break\n",
    "            jdata = json.loads(l)\n",
    "#             dt = time.time()\n",
    "            dict_token = dict_df[dict_df[\"token\"].isin(jdata[\"tokens\"])]\n",
    "            p_ = False\n",
    "            for _, r in dict_token.iterrows():\n",
    "                p_ = True\n",
    "                data.append(r.score)\n",
    "                indices.append(r.id)\n",
    "#             print(time.time() -dt)\n",
    "            if not p_:\n",
    "                continue\n",
    "            indptr.append(len(indices))\n",
    "\n",
    "#             print(\"id done %s, matrix id %s\" % (jdata[\"id\"], matrix_id))\n",
    "            map_context_id_to_matrix_id[jdata[\"id\"]] = matrix_id\n",
    "            b += 1\n",
    "            matrix_id += 1\n",
    "            if b == batch:\n",
    "                if indexer is None:\n",
    "                    indexer = sp.csr_matrix((data, indices, indptr), dtype=np.int8, shape=(len(indptr)-1,token_size))\n",
    "#                     print('done')\n",
    "                else:\n",
    "                    tmp = sp.csr_matrix((data, indices, indptr), dtype=np.int8, shape=(len(indptr)-1,token_size))\n",
    "                    indexer = sp.vstack((indexer, tmp), format='csr')\n",
    "#                     print('done')\n",
    "                indptr = [0]\n",
    "                indices = []\n",
    "                data = []\n",
    "                b = 0\n",
    "#                 print(\"done batch, matrix id %s, time %s\\n\" % (matrix_id, time.time()-t_point))\n",
    "    if b > 0:\n",
    "        if indexer is None:\n",
    "            indexer = sp.csr_matrix((data, indices, indptr), dtype=np.int8, shape=(len(indptr)-1,token_size))\n",
    "#             print(indexer.shape)\n",
    "        else:\n",
    "            tmp = sp.csr_matrix((data, indices, indptr), dtype=np.int8, shape=(len(indptr)-1,token_size))\n",
    "#             print(tmp.shape, indexer.shape)\n",
    "            indexer = sp.vstack((indexer, tmp), format='csr')\n",
    "        print(\"done batch, matrix id %s, time %s\\n\" % (matrix_id, time.time()-t_point))\n",
    "    print(\"done file, matrix id %s, time %s\\n\" % (matrix_id, time.time()-t_point))\n",
    "            \n",
    "    print(\"Save checkpoint\\n\")\n",
    "        \n",
    "    with open('map_context_id_to_matrix_id.pickle', 'wb') as handle:\n",
    "        pickle.dump(map_context_id_to_matrix_id, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    # save to file\n",
    "    print(\"Save indexer to file\\n\")\n",
    "    sp.save_npz(\"indexer.npz\", indexer, compressed=True)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "07183ec03ce540c22b5224a373e4bfad15798ca75676b2feb2f8165684becf10"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
