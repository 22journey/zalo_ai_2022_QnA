{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UpGYsb_qXIh1"
   },
   "source": [
    "# MRC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EAx6ZoGe-kgN",
    "outputId": "316d0f98-86f3-42d4-e7d6-8594754449bf"
   },
   "outputs": [],
   "source": [
    "!pip install transformers==4.17.0\n",
    "!git clone https://github.com/nguyenvulebinh/extractive-qa-mrc\n",
    "%cd ./extractive-qa-mrc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e7pNkZbqoLoL",
    "outputId": "32e321e4-7e1b-43ba-b19a-3bec5c8ebd24"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available(): \n",
    " dev = \"cuda:0\" \n",
    "else: \n",
    " dev = \"cpu\" \n",
    "print(dev)\n",
    "device = torch.device(dev) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uJ2MMBDz-yH6",
    "outputId": "43cd2849-ee1b-4810-acc7-d6e3aab7f290"
   },
   "outputs": [],
   "source": [
    "from infer import tokenize_function, data_collator, extract_answer\n",
    "from model.mrc_model import MRCQuestionAnswering\n",
    "from transformers import AutoTokenizer\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "llpd4O1T_Uh-"
   },
   "outputs": [],
   "source": [
    "model_checkpoint = \"nguyenvulebinh/vi-mrc-large\"\n",
    "# model_checkpoint = \"nguyenvulebinh/vi-mrc-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "model = MRCQuestionAnswering.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1KMhIIkiADaP",
    "outputId": "4a26d444-b93a-4925-d893-618f79440196"
   },
   "outputs": [],
   "source": [
    "QA_input = {\n",
    "  'question': \"Bình được công nhận với danh hiệu gì?\",\n",
    "  'context': \"Bình Nguyễn là một người đam mê với lĩnh vực xử lý ngôn ngữ tự nhiên. Anh được ghi danh là Google Developer Expert năm 2020\",\n",
    "  # 'question': \"Thủ tướng Trung Quốc là gì ?\",\n",
    "  # 'context': \"Thủ tướng Trung Quốc là nhân vật lãnh đạo chính phủ , chủ trì Quốc vụ viện gồm bốn phó thủ tướng cùng người đứng đầu các bộ và uỷ ban cấp bộ . Chủ tịch nước đương nhiệm là Tập Cận Bình , ông cũng là Tổng Bí thư của Đảng Cộng sản Trung Quốc và Chủ tịch Quân uỷ Trung Quốc , do vậy ông là lãnh đạo tối cao của Trung Quốc .\",\n",
    "  # 'question':\"Pháp tấn công xâm lược Việt Nam vào ngày tháng năm nào?\",\n",
    "  # 'context': \"Raymondienne (hay Raymonde Dien) sinh ngày 13 tháng 5 năm 1929 tại Pháp là một người phụ nữ (đảng viên Đảng Cộng sản Pháp) đã tham gia phong trào đấu tranh chống Pháp tái xâm lược Việt Nam.\"\n",
    "}\n",
    "\n",
    "inputs = [tokenize_function(QA_input, tokenizer)]\n",
    "inputs_ids = data_collator(inputs, tokenizer)\n",
    "outputs = model(**inputs_ids)\n",
    "answer = extract_answer(inputs, outputs, tokenizer)\n",
    "\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Az3AoHYTWCQV",
    "outputId": "10b8bbfc-17a3-40ae-92ca-f2d8dbea9564"
   },
   "outputs": [],
   "source": [
    "inputs = {\n",
    "    'question': 'Đạo diễn phim Titanic là ai',\n",
    "    # 'context': 'Titanic (phim)\\n\\nTitanic có thể là tên của các phim sau đây, theo thứ tự thời gian:\\n\\nBULLET::::- \"Titanic\" - năm 1943, đạo diễn: Werner Von Klingler\\nBULLET::::- \"Titanic\" - năm 1953, đạo diễn: Jean Negulesco\\nBULLET::::- \"Titanic\" - phim trên TV năm 1996, diễn viên: George C. Scott, Catherine Zeta-Jones, Pete Gallager, Tim Curry\\nBULLET::::- \"Titanic\" - năm 1997, đạo diễn: James Cameron, diễn viên: Leonardo DiCaprio, Kate Winslet\\n\\nNgoài ra đề tài về chuyến du hành của tàu RMS Titanic cũng là đề tài của các phim sau đây, theo thứ tự thời gian:\\nBULLET::::- \"In Nacht und Eis\" - năm 1912, phim Đức, phim đầu tiên về đề tài này\\nBULLET::::- \"Saved from the Titanic\" - năm 1912, diễn viên Dorothy Gibson trong phim cũng là một hành khách của tàu\\nBULLET::::- \"Atlantic\" - năm 1929, không dựa vào sự kiện lịch sử; sau được đổi tên thành \"Titanic: Disaster in the Atlantic\"\\nBULLET::::- \"A Night to Remember\" - năm 1958, hoàn toàn dựa vào sự kiện lịch sử của chiếc tàu này\\nBULLET::::- \"The Unsinkable Molly Brown\" - năm 1964, diễn viên Debbie Reynolds đóng vai hành khách Margaret Brown\\nBULLET::::- \"S.O.S. Titanic\" - phim trên TV năm 1979\\nBULLET::::- \"Raise the Titanic!\" - năm 1980, phim dựa vào truyện của Clive Cussler\\nBULLET::::- \"The Chambermaid on the Titanic\" - năm 1997, phim Pháp, đạo diễn: Bigas Luna.\\nBULLET::::- \"Ghosts of the Abyss\" - năm 2003, phim tài liệu, 3-D, đạo diễn: James Cameron\\n== Xem thêm. ==\\nBULLET::::- Titanic (định hướng)\\n',\n",
    "    'context': 'Giải Oscar lần thứ 70\\n\\nLễ trao giải Oscar lần thứ 70 được tổ chức để trao giải Oscar cho các tác phẩm xuất sắc trong lĩnh vực điện ảnh được ra mắt trong thời gian năm 1997 bởi Viện Hàn lâm Khoa học và Nghệ thuật Điện ảnh Hoa Kỳ, buổi lễ được tổ chức vào 23 tháng 3 năm 1998 tại thành phố Los Angeles. Lễ trao giải được tường thuật trực tiếp qua kênh ABC, nam diễn viên Billy Crystal đảm nhận vị trí dẫn dắt lần thứ 6. Vào 28 tháng 2 năm 1998, giải Oscar cho thanh tựu kỹ thuật được trao tại Beverly Hills.\\n\\n\"Titanic\" tạo nên một kỹ lục của giải mà cho tới hiện nay vẫn chứ bị phá vỡ khi giành tới 11 giải bao gồm Phim hay nhất và Đạo diễn xuất sắc nhất. Những giải còn lại được trao cho các phim \"As Good as It Gets\", \"Good Will Hunting\", \"L.A. Confidential\", \"The Full Monty\", \"Geri\\'s Game\", \"Karakter\", \"The Long Way Home\", \"Visas and Virtue\", \"Men in Black\" và \"A Story of Healing\".\\n\\n== Giải thưởng và đề cử. ==\\nCác đề cử của giải được công bố vào 10 tháng 2 năm 1998 bởi Robert Rehme, chủ tịch viện Hàn lâm và nữ diễn viên Geena Davis. \"Titanic\" giành số đề cử kỷ lục với 14 đề cử, tương đương với số đề cử All About Eve giành được vào năm 1950. \"Good Will Hunting\" và \"L.A. Confidential\" mỗi phim nhận được 9 đề cử.\\n\\nTại lễ trao giải, \"Titanic\" nhận được 11 tượng vàng, qua đó cùng với Ben-Hur là hai phim nhận nhiều giải Oscar nhất. \"Titanic\" là phim thứ hai giành giải phim hay nhất mà không nhận được đề cử ở các hạng mục dành cho kịch bản (The Sound of Music là phim đầu tiên). \"As Good as it Gets\" là phim thứ bảy mà cả hai vai chính đều nhận được giải. Kate Winslet và Gloria Stuart cùng giành đề cử ở hạng mục Nữ diễn viên chính xuất sắc nhất và Nữ diễn viên phụ xuất sắc nhất cho cùng một vai diễn Rose DeWitt Bukater trong \"Titanic\".\\n\\n=== Giải thưởng. ===\\nTên phim hoặc người nhận giải được in đậm\\n\\n=== Giải Oscar danh dự. ===\\nBULLET::::- Stanley Donen\\n\\n=== Phim với nhiều đề cử và giải thưởng. ===\\nDanh sách 16 phim nhận nhiều đề cử:\\n\\nCác phim nhận được nhiều giải thưởng:\\n\\n== Xem thêm. ==\\nBULLET::::- Giải SAG lần thứ 4\\nBULLET::::- Giải Grammy lần thứ 40\\nBULLET::::- Giải Mâm xôi vàng lần thứ 18\\nBULLET::::- Giải Primetime Emmy lần thứ 50\\nBULLET::::- Giải BAFTA lần thứ 51\\nBULLET::::- Giải Tony lần thứ 52\\nBULLET::::- Giải Quả cầu vàng lần thứ 55\\n\\n== Liên kết ngoài. ==\\nBULLET::::- Trang web chính thức\\n'\n",
    "    # 'question':\"Pháp tấn công xâm lược Việt Nam vào ngày tháng năm nào?\",\n",
    "    # 'context': \"Raymondienne (hay Raymonde Dien) sinh ngày 13 tháng 5 năm 1929 tại Pháp là một người phụ nữ (đảng viên Đảng Cộng sản Pháp) đã tham gia phong trào đấu tranh chống Pháp tái xâm lược Việt Nam.\",\n",
    "    # \"question\": \"Cờ vua còn có tên gọi nào khác\",\n",
    "    # \"context\": \"Cúp cờ vua thế giới là tên gọi một số giải đấu cờ vua khác nhau. Thể thức và ý nghĩa của các giải đấu này thay đổi theo thời gian.\",\n",
    "    # 'question':\"bác hồ ra đi tìm đường cứu nước năm nào\",\n",
    "    # \"context\":\"Ngày Bác Hồ ra đi tìm đường cứu nước (ngày 5 tháng 6 năm 1911) là ngày kỷ niệm hàng năm tại Việt Nam ghi nhận sự kiện Hồ Chí Minh (lúc đó tên là Nguyễn Tất Thành) rời bến Nhà Rồng trên con tàu Đô đốc Latouche-Tréville lên đường sang Pháp với tên gọi Văn Ba hay anh Ba[1][2] để học hỏi những điều mà ông cho là tinh hoa và tiến bộ từ các nước phương Tây nhằm thực hiện công cuộc giải phóng Việt Nam khỏi ách thuộc địa của Thực dân Pháp.Tại Việt Nam, ngày 5 tháng 6 hàng năm là một dịp lễ lớn cùng với nhiều hoạt động nhằm kỷ niệm, đặc biệt là các hoạt động được tổ chức rầm rộ trong năm 2011, là năm kỷ niệm tròn 100 năm sự kiện diễn ra.\"\n",
    "}\n",
    "\n",
    "inputs['context'] = inputs['context'][:1600]\n",
    "print(len(inputs['context']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pLdv5LfqWlP9",
    "outputId": "5e5aa0bd-c92b-4662-e058-0b6f6cad93c9"
   },
   "outputs": [],
   "source": [
    "inputs = [tokenize_function(inputs, tokenizer)]\n",
    "inputs_ids = data_collator(inputs, tokenizer)\n",
    "print(inputs_ids['input_ids'].shape)\n",
    "print(inputs_ids['attention_mask'].shape)\n",
    "print(inputs_ids['words_lengths'].shape)\n",
    "outputs = model(**inputs_ids)\n",
    "answer = extract_answer(inputs, outputs, tokenizer)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H6DwRxsXXMSV"
   },
   "source": [
    "# Infer zalo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iVw2X-1_YSL0",
    "outputId": "87f8fafe-a586-440a-8db3-f86c1e14146a"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0M-FWE68YfXe"
   },
   "outputs": [],
   "source": [
    "# !cp /content/drive/MyDrive/code/zac_qna_2022/zac2022_testa_only_question_with_context_full_16.jsonl /content/zac2022_test.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! cp out.jsonl out.jsonl.bk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 398
    },
    "id": "JwIMCBlhYv5U",
    "outputId": "a70f632e-f8c2-4b88-9964-5a0702633b2a"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import json\n",
    "import time\n",
    "import codecs\n",
    "\n",
    "predict_contexts = 16\n",
    "out = codecs.open(\"./out.jsonl\", \"w\", \"utf-8\")\n",
    "\n",
    "with open(\"./zac2022_test.jsonl\", \"r\") as f:\n",
    "    for _ in tqdm(range(600)):\n",
    "        line = f.readline()\n",
    "        data = json.loads(line)\n",
    "\n",
    "        question = data[\"question\"][\"question\"]\n",
    "\n",
    "        texts = list(map(lambda it: it[\"text\"], data[\"contexts\"]))[:predict_contexts]\n",
    "\n",
    "        t = time.time()\n",
    "\n",
    "        inputs_list = [{\n",
    "            \"question\": question,\n",
    "            \"context\": t[:1000],\n",
    "        } for t in texts]\n",
    "\n",
    "        answers = []\n",
    "\n",
    "        for i in range(len(inputs_list)):\n",
    "            inputs = inputs_list[i]\n",
    "            # print(inputs)\n",
    "            \n",
    "            inputs = [tokenize_function(inputs, tokenizer)]\n",
    "            inputs_ids = data_collator(inputs, tokenizer)\n",
    "            outputs = model(**inputs_ids)\n",
    "            answer = extract_answer(inputs, outputs, tokenizer)[0]\n",
    "            answers.append([answer[\"answer\"], answer[\"score_start\"]])\n",
    "\n",
    "        best_pos, best_score = -1, -1\n",
    "        for i in range(len(answers)):\n",
    "            if answers[i][0] != '' and answers[i][1] > best_score:\n",
    "                best_score, best_pos = answers[i][1], i\n",
    "        \n",
    "        # print(\"Final result: \")\n",
    "        # print(\"Question: \", question)\n",
    "        if best_pos >= 0:\n",
    "            # print(\"id: \", data[\"contexts\"][best_pos][\"id\"])\n",
    "            # print(\"answer: \", answers[best_pos])\n",
    "            # print(\"context_loc: \", best_pos)\n",
    "            predict = {\n",
    "                \"answer\": answers[best_pos][0],\n",
    "                \"score\": answers[best_pos][1],\n",
    "                \"context\": data[\"contexts\"][best_pos][\"id\"],\n",
    "                \"context_title\": data[\"contexts\"][best_pos][\"title\"],\n",
    "            }\n",
    "        else:\n",
    "            # print(\"answer: null\")\n",
    "            predict = None\n",
    "\n",
    "        dt = time.time() - t\n",
    "        # print(\"Time: \", dt)\n",
    "\n",
    "        o = {\n",
    "            \"id\": data[\"id\"],\n",
    "            \"question\": data[\"question\"],\n",
    "            \"predict\": predict,\n",
    "            \"time_predict\": dt,\n",
    "            \"context_loc\": best_pos,\n",
    "            \"score\": best_score,\n",
    "        }\n",
    "        out.write(json.dumps(o, ensure_ascii=False))\n",
    "        out.write(\"\\n\")\n",
    "        out.flush()\n",
    "        \n",
    "out.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './extractive-qa-mrc/out17112022012438.jsonl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [88], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m file \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m./extractive-qa-mrc/out\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m.jsonl\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m file_suffix\n\u001b[1;32m     10\u001b[0m q, \u001b[39mid\u001b[39m, answer, title, context_loc, score \u001b[39m=\u001b[39m [], [], [], [], [], []\n\u001b[0;32m---> 11\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39;49m(file, \u001b[39m'\u001b[39;49m\u001b[39mr\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39mas\u001b[39;00m f:\n\u001b[1;32m     12\u001b[0m     \u001b[39mfor\u001b[39;00m l \u001b[39min\u001b[39;00m f:\n\u001b[1;32m     13\u001b[0m         d \u001b[39m=\u001b[39m json\u001b[39m.\u001b[39mloads(l)\n",
      "File \u001b[0;32m~/miniconda3/envs/pt_py3.8/lib/python3.8/site-packages/IPython/core/interactiveshell.py:282\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[39mif\u001b[39;00m file \u001b[39min\u001b[39;00m {\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m}:\n\u001b[1;32m    276\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    277\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mIPython won\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt let you open fd=\u001b[39m\u001b[39m{\u001b[39;00mfile\u001b[39m}\u001b[39;00m\u001b[39m by default \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    278\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    279\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39myou can use builtins\u001b[39m\u001b[39m'\u001b[39m\u001b[39m open.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    280\u001b[0m     )\n\u001b[0;32m--> 282\u001b[0m \u001b[39mreturn\u001b[39;00m io_open(file, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './extractive-qa-mrc/out17112022012438.jsonl'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import datetime\n",
    "\n",
    "file_suffix = datetime.datetime.now().strftime(\"%d%m%Y%H%M%S\")\n",
    "\n",
    "file = \"./extractive-qa-mrc/out%s.jsonl\" % file_suffix\n",
    "\n",
    "q, id, answer, title, context_loc, score = [], [], [], [], [], []\n",
    "with open(file, 'r') as f:\n",
    "    for l in f:\n",
    "        d = json.loads(l)\n",
    "        q.append(d[\"question\"][\"question\"])\n",
    "        id.append(d[\"id\"])\n",
    "        context_loc.append(d[\"context_loc\"])\n",
    "        score.append(d[\"score\"])\n",
    "        if d[\"predict\"]:\n",
    "            answer.append(d[\"predict\"][\"answer\"])\n",
    "            title.append(d[\"predict\"][\"context_title\"])\n",
    "        else:\n",
    "            answer.append(\"\")\n",
    "            title.append(\"\")\n",
    "        \n",
    "df = pd.DataFrame.from_dict({\n",
    "    \"question\": q,\n",
    "    \"id\": id,\n",
    "    \"answer\": answer,\n",
    "    \"title\": title,\n",
    "    \"context_loc\": context_loc,\n",
    "    \"answer_type\": \"wiki\",\n",
    "    \"score\": score,\n",
    "})\n",
    "\n",
    "# df.to_csv(\"output_cleaned_%s.csv\" % suffix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with pd.option_context('display.max_rows', None,):\n",
    "#     print(df[df[\"answer\"].str.contains(\"năm|tháng|ngày|/\")].answer)\n",
    "# df[df[\"score\"] > 0].describe()\n",
    "\n",
    "# df = pd.read_csv(\"output_cleaned.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import re\n",
    "\n",
    "file_suffix = datetime.datetime.now().strftime(\"%d%m%Y%H%M%S\")\n",
    "\n",
    "file = \"./extractive-qa-mrc/out%s.jsonl\" % file_suffix\n",
    "\n",
    "# clean date data\n",
    "try_formats = [\n",
    "    (\"%d/%m/%Y\", (1,1,1)),\n",
    "#     (\"%-d/%-m/%Y\", (1, 1, 1)),\n",
    "#     (\"%-d/%-m\", (1, 1, 0)),\n",
    "    (\"%d/%m\", (1, 1, 0)),\n",
    "    (\"%m/%Y\", (0, 1, 1)),\n",
    "#     (\"%d tháng %m năm %Y\", (1, 1, 1)),\n",
    "]\n",
    "\n",
    "def try_parse_date_format(s):\n",
    "    for tryf, mask in try_formats:\n",
    "        try:\n",
    "            d = datetime.datetime.strptime(s, tryf)\n",
    "#             print(d)\n",
    "            outp = []\n",
    "            if mask[0] == 1:\n",
    "                outp.append(\"ngày \"+str(d.day))\n",
    "            if mask[1] == 1:\n",
    "                outp.append(\"tháng \"+str(d.month))\n",
    "            if mask[2] == 1:\n",
    "                outp.append(\"năm \"+str(d.year))\n",
    "            return True, \" \".join(outp)\n",
    "        except Exception as e:\n",
    "#             print(e)\n",
    "            continue\n",
    "    return False, None\n",
    "\n",
    "def try_format_date(s):\n",
    "    # clean first\n",
    "    s = s.lower()\n",
    "    s = s.replace(\"mùng\", \"ngày\")\n",
    "    # for case 14 tháng 3 năm 1883\n",
    "    s = \"ngày \" + s\n",
    "    ok = False\n",
    "    m = re.findall(r\"((ngày|tháng|năm) [\\d/]+)+\", s)\n",
    "    if m:\n",
    "        s = \" \".join([it[0] for it in m])\n",
    "        ok = True\n",
    "\n",
    "    m = re.match(r\"(ngày|tháng) ((\\d+\\/)+\\d+)\", s)\n",
    "    if m:\n",
    "        s = m.group(2)\n",
    "    ok_2, datestring = try_parse_date_format(s)\n",
    "    if ok_2:\n",
    "        return True, datestring\n",
    "    \n",
    "    return ok, s \n",
    "\n",
    "datedf = df[df[\"answer\"].str.contains(\"Năm|Tháng|Ngày|năm|tháng|ngày|/\")]\n",
    "for index, row in datedf.iterrows():\n",
    "    ok, datestring = try_format_date(row.answer)\n",
    "    if ok:\n",
    "        datedf.at[index, \"answer\"] = datestring\n",
    "        datedf.at[index, \"answer_type\"] = \"date\"\n",
    "        \n",
    "# with pd.option_context('display.max_rows', None,):\n",
    "#     print(datedf.answer)\n",
    "    \n",
    "df.update(datedf)\n",
    "\n",
    "# print(try_format_date(\"1/11/2012\"))\n",
    "# print(try_format_date(\"11/2012\"))\n",
    "# print(try_format_date(\"1/11\"))\n",
    "# print(try_format_date(\"ngày 2/11\"))\n",
    "# print(try_format_date(\"tháng 1/2000\"))\n",
    "# print(try_format_date(\"ngày 9 tháng 11 năm 1946\"))\n",
    "# print(try_format_date(\"mùng 9 tháng 11 năm 1946\"))\n",
    "# print(try_format_date(\"năm 1982 tại New Delhi , Ấn Độ\"))\n",
    "# print(try_format_date(\"đầu năm 1979\"))\n",
    "# print(try_format_date(\"14 tháng 3 năm 1883\"))\n",
    "\n",
    "# format number\n",
    "df_number = df[(df[\"answer\"].str.contains(\"[1234567890]\"))&(~df[\"answer\"].str.contains(\"ngày|tháng|năm\"))]\n",
    "for index, row in df_number.iterrows():\n",
    "    m = re.match(r\"^(\\d+) .*$\", row.answer)\n",
    "    if m:\n",
    "        df_number.at[index, \"answer\"] = m.group(1)\n",
    "        df_number.at[index, \"answer_type\"] = \"number\"\n",
    "    try:\n",
    "        int(row.answer)\n",
    "        if re.search(r\"(năm bao nhiêu|năm nào|năm mấy)+\", row.question):\n",
    "            print(\"match\")\n",
    "            df_number.at[index, \"answer\"] = \"năm %s\" % row.answer\n",
    "            df_number.at[index, \"answer_type\"] = \"date\"\n",
    "        else:\n",
    "            df_number.at[index, \"answer_type\"] = \"number\"\n",
    "    except Exception as e:\n",
    "        pass\n",
    "df.update(df_number)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# m = re.search(r\"(năm bao nhiêu|năm nào)+\", \"Nguyễn Kim Thành sinh năm nào\")\n",
    "# print(m)\n",
    "\n",
    "df.to_csv(\"./output_cleaned_%s.csv\" % file_suffix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.option_context('display.max_rows', None,):\n",
    "    print(df[(df[\"answer\"].str.contains(\"[1234567890]\"))&(~df[\"answer\"].str.contains(\"ngày|tháng|năm\"))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "els_index = \"wikipedia_29220620_titles\"\n",
    "els = Elasticsearch(\"http://localhost:9200\")\n",
    "\n",
    "def find_wiki_title(answer, size=10):\n",
    "    resp = els.search(\n",
    "        index=els_index, \n",
    "        query={\n",
    "            \"match\": {\n",
    "                \"title\": {\n",
    "                    \"query\": answer\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "        size=size,\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        return [it[\"_source\"][\"title\"] for it in resp[\"hits\"][\"hits\"]]\n",
    "        # for item in resp[\"hits\"][\"hits\"]:\n",
    "        #     yield item[\"_source\"]\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>question</th>\n",
       "      <th>id</th>\n",
       "      <th>answer</th>\n",
       "      <th>title</th>\n",
       "      <th>context_loc</th>\n",
       "      <th>answer_type</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Đạo diễn phim Titanic là ai</td>\n",
       "      <td>testa_1</td>\n",
       "      <td>James Cameron</td>\n",
       "      <td>James Cameron</td>\n",
       "      <td>15.0</td>\n",
       "      <td>wiki</td>\n",
       "      <td>0.999993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Tổng thống Hoa Kỳ thứ 45 là ai</td>\n",
       "      <td>testa_2</td>\n",
       "      <td>Donald Trump</td>\n",
       "      <td>Mary Anne MacLeod Trump</td>\n",
       "      <td>7.0</td>\n",
       "      <td>wiki</td>\n",
       "      <td>0.999980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Hiện nay ai là tổng bí thư nước Việt Nam</td>\n",
       "      <td>testa_3</td>\n",
       "      <td>Vũ Việt Trang</td>\n",
       "      <td>Vũ Việt Trang</td>\n",
       "      <td>9.0</td>\n",
       "      <td>wiki</td>\n",
       "      <td>0.999454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>đâu là bản hiến pháp lâu đời nhất thế giới</td>\n",
       "      <td>testa_4</td>\n",
       "      <td>Hiến pháp Canada</td>\n",
       "      <td>Hiến pháp Canada</td>\n",
       "      <td>5.0</td>\n",
       "      <td>wiki</td>\n",
       "      <td>0.998735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Tổ chức thống nhất châu Phi được thành lập ở đâu</td>\n",
       "      <td>testa_5</td>\n",
       "      <td>Addis Ababa</td>\n",
       "      <td>Tổ chức châu Phi Thống nhất</td>\n",
       "      <td>1.0</td>\n",
       "      <td>wiki</td>\n",
       "      <td>0.999994</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                          question       id  \\\n",
       "0           0                       Đạo diễn phim Titanic là ai  testa_1   \n",
       "1           1                    Tổng thống Hoa Kỳ thứ 45 là ai  testa_2   \n",
       "2           2          Hiện nay ai là tổng bí thư nước Việt Nam  testa_3   \n",
       "3           3        đâu là bản hiến pháp lâu đời nhất thế giới  testa_4   \n",
       "4           4  Tổ chức thống nhất châu Phi được thành lập ở đâu  testa_5   \n",
       "\n",
       "             answer                        title  context_loc answer_type  \\\n",
       "0     James Cameron                James Cameron         15.0        wiki   \n",
       "1      Donald Trump      Mary Anne MacLeod Trump          7.0        wiki   \n",
       "2     Vũ Việt Trang                Vũ Việt Trang          9.0        wiki   \n",
       "3  Hiến pháp Canada             Hiến pháp Canada          5.0        wiki   \n",
       "4       Addis Ababa  Tổ chức châu Phi Thống nhất          1.0        wiki   \n",
       "\n",
       "      score  \n",
       "0  0.999993  \n",
       "1  0.999980  \n",
       "2  0.999454  \n",
       "3  0.998735  \n",
       "4  0.999994  "
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"output_cleaned.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_23461/2715089495.py:6: ElasticsearchWarning: Elasticsearch built-in security features are not enabled. Without authentication, your cluster could be accessible to anyone. See https://www.elastic.co/guide/en/elasticsearch/reference/7.17/security-minimal-setup.html to enable security.\n",
      "  resp = els.search(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1\n",
      "1\n",
      "4\n",
      "1\n",
      "2\n",
      "2\n",
      "1\n",
      "3\n",
      "1\n",
      "1\n",
      "3\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "1\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "11\n",
      "2\n",
      "1\n",
      "2\n",
      "2\n",
      "2\n",
      "5\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "1\n",
      "3\n",
      "2\n",
      "1\n",
      "2\n",
      "1\n",
      "4\n",
      "1\n",
      "2\n",
      "1\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "5\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "2\n",
      "1\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "1\n",
      "1\n",
      "3\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "2\n",
      "1\n",
      "1\n",
      "2\n",
      "1\n",
      "1\n",
      "2\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "1\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "1\n",
      "2\n",
      "1\n",
      "3\n",
      "1\n",
      "3\n",
      "2\n",
      "1\n",
      "1\n",
      "2\n",
      "1\n",
      "1\n",
      "2\n",
      "1\n",
      "10\n",
      "2\n",
      "1\n",
      "3\n",
      "3\n",
      "4\n",
      "1\n",
      "1\n",
      "3\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "1\n",
      "2\n",
      "1\n",
      "4\n",
      "1\n",
      "1\n",
      "2\n",
      "1\n",
      "5\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "1\n",
      "1\n",
      "2\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "3\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "submit_17112022210226.json\n"
     ]
    }
   ],
   "source": [
    "# output submission\n",
    "\n",
    "import datetime\n",
    "\n",
    "file_suffix = datetime.datetime.now().strftime(\"%d%m%Y%H%M%S\")\n",
    "\n",
    "data = []\n",
    "for index, row in df.iterrows():\n",
    "    candidate = []\n",
    "    raw_answer = row.answer\n",
    "    if row.answer == \"\":\n",
    "        a = None\n",
    "    elif row.answer_type in [\"date\", \"number\"]:\n",
    "        a = row.answer\n",
    "    else:\n",
    "        # candidate = find_wiki_title(row.answer)\n",
    "        # if candidate:\n",
    "        #     entity = candidate[0]\n",
    "        # else:\n",
    "        #     entity = row.title\n",
    "        # a = \"wiki/\" + entity.replace(\" \", \"_\")\n",
    "        try:\n",
    "            entity, candidate = get_wiki_entry(row.answer, row.question)\n",
    "            raw_answer = row.answer\n",
    "            if entity:\n",
    "                a = \"wiki/\" + entity.replace(\" \", \"_\")\n",
    "            else:\n",
    "                a = None\n",
    "        except:\n",
    "            a = None\n",
    "            raw_answer = None\n",
    "    data.append({\n",
    "        \"id\": row.id,\n",
    "        \"question\": row.question,\n",
    "        \"answer\": a,\n",
    "        \"answer_candidate\": candidate,\n",
    "        \"score\": row.score,\n",
    "        \"raw_answer\": raw_answer,\n",
    "    })\n",
    "    \n",
    "import json\n",
    "import codecs\n",
    "\n",
    "out_filename = \"submit_%s.json\" % file_suffix\n",
    "\n",
    "with codecs.open(out_filename, \"w\", \"utf-8\") as f:\n",
    "    f.write(json.dumps({\n",
    "        \"data\": data,\n",
    "    }, indent=4, ensure_ascii=False))\n",
    "    \n",
    "print(out_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean wiki entity answer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv(\"output_cleaned.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>id</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Lưu Bị sống ở thời kỳ lịch sử nào của Trung Quốc</td>\n",
       "      <td>testa_21</td>\n",
       "      <td>thời Tam Quốc trong lịch sử Trung Quốc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Kinh Dương Vương có quan hệ như thế nào với Lạ...</td>\n",
       "      <td>testa_29</td>\n",
       "      <td>là dòng dõi rồng sống dưới nước</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Ngôi sao sáng nhất trên bầu trời đêm ?</td>\n",
       "      <td>testa_34</td>\n",
       "      <td>Sirius</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>Trụ sở chính của Google tên là gì</td>\n",
       "      <td>testa_37</td>\n",
       "      <td>Mountain View</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>Thành phố nào hiện là tỉnh lỵ của Kon Tum</td>\n",
       "      <td>testa_44</td>\n",
       "      <td>thành phố Kon Tum</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>475</th>\n",
       "      <td>Ai là hoàng đế đầu tiên sau thời Bắc Thuộc?</td>\n",
       "      <td>testa_476</td>\n",
       "      <td>Cao Diễn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>486</th>\n",
       "      <td>Vị tổng thống đầu tiên của nước Nga là ai ?</td>\n",
       "      <td>testa_487</td>\n",
       "      <td>Boris Yeltsin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>491</th>\n",
       "      <td>Thủ đô của Campuchia là thành phố nào</td>\n",
       "      <td>testa_492</td>\n",
       "      <td>Phnom Penh</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>524</th>\n",
       "      <td>Cây thông Giáng sinh hiện đại được cho là có n...</td>\n",
       "      <td>testa_525</td>\n",
       "      <td>nước Đức</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>577</th>\n",
       "      <td>Truyền thuyết về vua Hùng bắt nguồn từ thị trấ...</td>\n",
       "      <td>testa_578</td>\n",
       "      <td>thị trấn Hùng Sơn</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>71 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              question         id  \\\n",
       "20    Lưu Bị sống ở thời kỳ lịch sử nào của Trung Quốc   testa_21   \n",
       "28   Kinh Dương Vương có quan hệ như thế nào với Lạ...   testa_29   \n",
       "33              Ngôi sao sáng nhất trên bầu trời đêm ?   testa_34   \n",
       "36                   Trụ sở chính của Google tên là gì   testa_37   \n",
       "43           Thành phố nào hiện là tỉnh lỵ của Kon Tum   testa_44   \n",
       "..                                                 ...        ...   \n",
       "475        Ai là hoàng đế đầu tiên sau thời Bắc Thuộc?  testa_476   \n",
       "486        Vị tổng thống đầu tiên của nước Nga là ai ?  testa_487   \n",
       "491              Thủ đô của Campuchia là thành phố nào  testa_492   \n",
       "524  Cây thông Giáng sinh hiện đại được cho là có n...  testa_525   \n",
       "577  Truyền thuyết về vua Hùng bắt nguồn từ thị trấ...  testa_578   \n",
       "\n",
       "                                     answer  \n",
       "20   thời Tam Quốc trong lịch sử Trung Quốc  \n",
       "28          là dòng dõi rồng sống dưới nước  \n",
       "33                                   Sirius  \n",
       "36                            Mountain View  \n",
       "43                        thành phố Kon Tum  \n",
       "..                                      ...  \n",
       "475                                Cao Diễn  \n",
       "486                           Boris Yeltsin  \n",
       "491                              Phnom Penh  \n",
       "524                                nước Đức  \n",
       "577                       thị trấn Hùng Sơn  \n",
       "\n",
       "[71 rows x 3 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show fail\n",
    "fail_list = \"\"\"\n",
    "testa_21\n",
    "testa_29\n",
    "testa_34\n",
    "testa_37\n",
    "testa_44\n",
    "testa_53\n",
    "testa_56\n",
    "testa_66\n",
    "testa_69\n",
    "testa_73\n",
    "testa_76\n",
    "testa_80\n",
    "testa_82\n",
    "testa_95\n",
    "testa_96\n",
    "testa_108\n",
    "testa_114\n",
    "testa_117\n",
    "testa_122\n",
    "testa_123\n",
    "testa_141\n",
    "testa_150\n",
    "testa_157\n",
    "testa_159\n",
    "testa_167\n",
    "testa_171\n",
    "testa_174\n",
    "testa_178\n",
    "testa_185\n",
    "testa_186\n",
    "testa_190\n",
    "testa_192\n",
    "testa_204\n",
    "testa_207\n",
    "testa_210\n",
    "testa_211\n",
    "testa_226\n",
    "testa_230\n",
    "testa_237\n",
    "testa_240\n",
    "testa_252\n",
    "testa_255\n",
    "testa_259\n",
    "testa_263\n",
    "testa_267\n",
    "testa_271\n",
    "testa_280\n",
    "testa_288\n",
    "testa_304\n",
    "testa_307\n",
    "testa_342\n",
    "testa_346\n",
    "testa_352\n",
    "testa_353\n",
    "testa_359\n",
    "testa_364\n",
    "testa_368\n",
    "testa_379\n",
    "testa_417\n",
    "testa_422\n",
    "testa_426\n",
    "testa_439\n",
    "testa_440\n",
    "testa_443\n",
    "testa_461\n",
    "testa_467\n",
    "testa_476\n",
    "testa_487\n",
    "testa_492\n",
    "testa_525\n",
    "testa_578\n",
    "\"\"\"\n",
    "\n",
    "fail_list = [s.strip() for s in fail_list.splitlines() if s.strip()]\n",
    "\n",
    "df[df.id.isin(fail_list)][[\"question\", \"id\", \"answer\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [answer]\n",
       "Index: []"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# threshold cutoff\n",
    "df[(df.score < 0.5)&(df.score>0)][\"answer\"] = \"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_23461/2715089495.py:6: ElasticsearchWarning: Elasticsearch built-in security features are not enabled. Without authentication, your cluster could be accessible to anyone. See https://www.elastic.co/guide/en/elasticsearch/reference/7.17/security-minimal-setup.html to enable security.\n",
      "  resp = els.search(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: thời Tam Quốc trong lịch sử Trung Quốc, title Tam quốc\n",
      "\n",
      "\n",
      "Q: thành phố Kon Tum, title Kon Tum (thành phố)\n",
      "\n",
      "\n",
      "Q: nước Đức, title Đức\n",
      "\n",
      "\n",
      "Q: thị trấn Hùng Sơn, title None\n",
      "\n",
      "\n",
      "Q: sa mạc Arabia, title 1157 Arabia\n",
      "\n",
      "\n",
      "Q: Bắc Triều Tiên, title Lịch Bắc Triều Tiên\n",
      "\n",
      "\n",
      "Q: Nguyễn Thành Trung, title Nguyễn Thành Trung\n",
      "\n",
      "\n",
      "Lý Bí\n"
     ]
    }
   ],
   "source": [
    "# find best relevent title\n",
    "def load_stopwords():\n",
    "    stopwords_filename = [\"./stop_words.txt\"]\n",
    "    stopwords = []\n",
    "    for sw in stopwords_filename:\n",
    "        with open(sw, \"r\") as f:\n",
    "            stopwords += [l.strip() for l in f]\n",
    "    stopwords = set(stopwords)\n",
    "    return stopwords\n",
    "stopwords = load_stopwords()\n",
    "\n",
    "\n",
    "def generateNgram(paper, ngram = 2, deli = '_', rmSet = {}):\n",
    "    words = paper.split()\n",
    "    if len(words) == 1:\n",
    "        return ''\n",
    "    \n",
    "    ngrams = []\n",
    "    for i in range(0,len(words) - ngram + 1):\n",
    "        block = words[i:i + ngram]\n",
    "        if not any((w in rmSet) for w in block):\n",
    "            ngrams.append(deli.join(block))\n",
    "    return ngrams\n",
    "\n",
    "ss = [\n",
    "    \"thời Tam Quốc trong lịch sử Trung Quốc\",\n",
    "    \"thành phố Kon Tum\",\n",
    "    \"nước Đức\",\n",
    "    \"thị trấn Hùng Sơn\",\n",
    "    \"sa mạc Arabia\",\n",
    "    \"Bắc Triều Tiên\",\n",
    "    \"Nguyễn Thành Trung\",\n",
    "]\n",
    "\n",
    "qq = [\n",
    "    \"Lưu Bị sống ở thời kỳ lịch sử nào của Trung Quốc\",\n",
    "    \"Thành phố nào hiện là tỉnh lỵ của Kon Tum\",\n",
    "    \"Cây thông Giáng sinh hiện đại được cho là có nguồn gốc từ nước nào\",\n",
    "    \"Truyền thuyết về vua Hùng bắt nguồn từ thị trấn nào ngày nay?\",\n",
    "    \"Phần lớn lãnh thổ Jordan bị bao phủ bởi sa mạc nào\",\n",
    "    \"Hàn Quốc có biên giới trên bộ giáp với nước nào\",\n",
    "    \"Ai là người lái máy bay ném bom vào dinh độc lập ?\",\n",
    "]\n",
    "\n",
    "from underthesea import text_normalize, word_tokenize\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    # text = text.translate(str.maketrans('', '', r\"-–,\\(\\)\"))\n",
    "    text = text.translate(str.maketrans('', '', r\",\\(\\)\"))\n",
    "    return text\n",
    "\n",
    "# for s, q in zip(ss, qq):\n",
    "#     s = clean_text(s)\n",
    "#     text = word_tokenize(s)\n",
    "#     text = list(filter(lambda it: it not in stopwords, text))\n",
    "#     print(text)\n",
    "\n",
    "#     rel = find_wiki_title(s)\n",
    "#     # print(rel)\n",
    "#     # print(\"\\n\")\n",
    "\n",
    "#     # cutoff tokens\n",
    "#     cutoff = list(filter(lambda it: it.lower() not in q.lower(), text))\n",
    "    # if cutoff:\n",
    "    #     rel2 = find_wiki_title(' '.join(cutoff))\n",
    "    #     rel = rel2 + rel\n",
    "    #     # print(rel)\n",
    "    #     # print(\"\\n\")\n",
    "\n",
    "    # rel = list(map(lambda it: clean_text(it), rel))\n",
    "    # rel_bigram = list(map(lambda it: generateNgram(it), rel))\n",
    "    # print(rel)\n",
    "    # print(rel_bigram)\n",
    "\n",
    "    # query = ' '.join(text)\n",
    "    # query_bigram = generateNgram(query)\n",
    "    # print(query)\n",
    "    # print(query_bigram)\n",
    "\n",
    "    # print(\"\\n\")\n",
    "\n",
    "def get_wiki_entry(answer, question):\n",
    "    s, q = answer, question\n",
    "    s = clean_text(s)\n",
    "    s = word_tokenize(s)\n",
    "    s = list(filter(lambda it: it not in stopwords, s))\n",
    "    s = ' '.join(s)\n",
    "\n",
    "    q = clean_text(q)\n",
    "    q = word_tokenize(q)\n",
    "    q = list(filter(lambda it: it not in stopwords, q))\n",
    "    q = ' '.join(q)\n",
    "\n",
    "    s_bigram = generateNgram(s)\n",
    "    q_bigram = generateNgram(q)\n",
    "\n",
    "    # print(s_bigram)\n",
    "    # print(q_bigram)\n",
    "\n",
    "    remove_idx = []\n",
    "    for idx, sb in enumerate(s_bigram):\n",
    "        if sb in q_bigram:\n",
    "            remove_idx += [idx,]\n",
    "            if idx > 0:\n",
    "                remove_idx += [idx-1, ]\n",
    "            if idx+1 < len(s_bigram):\n",
    "                remove_idx += [idx+1, ]\n",
    "\n",
    "    s2_bigram = [it for idx, it in enumerate(s_bigram) if idx not in remove_idx]\n",
    "    s2 = ' '.join(s2_bigram).replace(\"_\", \" \")\n",
    "    if s2 == \"\":\n",
    "        s2 = s\n",
    "        s2_bigram = s_bigram\n",
    "    # print(s2)\n",
    "    # print(s2_bigram)\n",
    "\n",
    "    rel = find_wiki_title(s)\n",
    "    # print(rel)\n",
    "\n",
    "    rel_bigram = list(map(lambda it: generateNgram(it), list(map(lambda it: clean_text(it), rel))))\n",
    "    # print(rel_bigram)\n",
    "\n",
    "    title = None\n",
    "\n",
    "    # find match \n",
    "    rel_lower = list(map(lambda it: it.lower(), rel))\n",
    "    if s in rel_lower:\n",
    "        title = rel[rel_lower.index(s)]\n",
    "    else:\n",
    "        # find bigram rel\n",
    "        rel_score = list(map(\n",
    "            lambda it: len(set(it) & set(s2_bigram))/max(len(set(it)), len(set(s2_bigram))),\n",
    "            rel_bigram,\n",
    "        ))\n",
    "        sorted_score = np.argsort([-s for s in rel_score])\n",
    "        _s = []\n",
    "        _rel = []\n",
    "        for i in sorted_score:\n",
    "            _s.append(rel_score[i])\n",
    "            _rel.append(rel[i])\n",
    "        # print(_rel)\n",
    "        # print(_s)\n",
    "        # if rel_score[sorted_score[0]] > 0 and (len(sorted_score)==1 or rel_score[sorted_score[0]] > rel_score[sorted_score[1]]):\n",
    "        if rel_score[sorted_score[0]] > 0:\n",
    "            title = rel[sorted_score[0]]\n",
    "\n",
    "    # print(\"Q: %s, title %s\" % (s, title))\n",
    "    # print(\"\\n\")\n",
    "    return title, rel\n",
    "\n",
    "def get_wiki_entry(answer, question):\n",
    "    s, q = answer, question\n",
    "\n",
    "    s = clean_text(s)\n",
    "    s_tokens = word_tokenize(s)\n",
    "    s_tokens = list(filter(lambda it: it not in stopwords, s_tokens))\n",
    "    s = ' '.join(s_tokens)\n",
    "    \n",
    "    def count_exist(tokens, text):\n",
    "        c = 0\n",
    "        for t in tokens:\n",
    "            if t in text:\n",
    "                c += 1\n",
    "        return c\n",
    "\n",
    "    rel = find_wiki_title(s, size=20)\n",
    "    rel_find = list(map(clean_text, rel))\n",
    "\n",
    "    # find equal\n",
    "    title = next((it for it in rel if it.lower() == s.lower()), None)\n",
    "    if title:\n",
    "        return title, rel\n",
    "\n",
    "    # score\n",
    "    is_in = [count_exist(s_tokens, it) for it in rel_find]\n",
    "    score = np.array(is_in) * 3\n",
    "    print(score)\n",
    "    return rel[0], rel\n",
    "\n",
    "\n",
    "def clean_text(text):\n",
    "    # text = text.lower()\n",
    "    # text = text.translate(str.maketrans('', '', r\"-–,\\(\\)\"))\n",
    "    text = text.translate(str.maketrans('', '', r\",\\(\\)\"))\n",
    "    return text\n",
    "\n",
    "\n",
    "def remove_duplicate_result(text):\n",
    "    splitter = text.strip().split(\" \")\n",
    "    if len(splitter) % 2 == 1:\n",
    "        return text\n",
    "    middle = int(len(splitter)/2)\n",
    "    # print(middle)\n",
    "    if splitter[:middle] == splitter[middle:]:\n",
    "        return ' '.join(splitter[:middle])\n",
    "    return text\n",
    "\n",
    "\n",
    "def get_wiki_entry(answer, question):\n",
    "    answer = remove_duplicate_result(answer)\n",
    "    \n",
    "    _n_exists_in_q_score = 3\n",
    "    _entity_score = 2\n",
    "    _multiword_factor = 2\n",
    "    _miss_score = -2\n",
    "\n",
    "    s = answer\n",
    "\n",
    "    s = clean_text(s)\n",
    "    s_tokens = word_tokenize(s)\n",
    "    s_tokens = list(filter(lambda it: it not in stopwords, s_tokens))\n",
    "    s = ' '.join(s_tokens)\n",
    "\n",
    "    q = clean_text(question)\n",
    "    q = q.lower()\n",
    "\n",
    "    rel = find_wiki_title(s, size=20)\n",
    "    rel_clean = list(map(lambda it: clean_text(it.lower()), rel))\n",
    "\n",
    "    # find equal\n",
    "    title = next((it for it in rel if it.lower() == s.lower()), None)\n",
    "    if title:\n",
    "        return title, rel\n",
    "\n",
    "    # score token\n",
    "    score_token = [1]*len(s_tokens)\n",
    "    \n",
    "    for i in range(len(score_token)):\n",
    "        if s_tokens[i].lower() not in q:\n",
    "            score_token[i] += _n_exists_in_q_score\n",
    "        if s_tokens[i].isupper():\n",
    "            score_token[i] += _entity_score\n",
    "        if len(s_tokens[i]) > 1:\n",
    "            score_token[i] *= _multiword_factor\n",
    "\n",
    "    # print(score_token)\n",
    "    # print(s_tokens)\n",
    "\n",
    "    s_tokens = list(map(lambda it: it.lower(), s_tokens))\n",
    "    # score all rel\n",
    "    score_rel =[0]*len(rel)\n",
    "    total_score = sum(score_token)\n",
    "    for i in range(len(score_rel)):\n",
    "        for ti, t in enumerate(s_tokens):\n",
    "            if t in rel_clean[i]:\n",
    "                score_rel[i] += score_token[ti]\n",
    "\n",
    "        rel_token = word_tokenize(rel_clean[i])\n",
    "        score_rel[i] += _miss_score * sum([0 if it in s.lower() else 1 for it in rel_token])\n",
    "        \n",
    "    \n",
    "    # print(score_rel)\n",
    "    # print(rel)\n",
    "    sorted_score = np.argsort([-s for s in score_rel])\n",
    "    # print(rel[sorted_score[0]])\n",
    "    # print(score_rel[sorted_score[0]])\n",
    "    best_pos = sorted_score[0]\n",
    "    if score_rel[best_pos] <= 0:\n",
    "        return None, rel\n",
    "\n",
    "    return rel[best_pos], rel\n",
    "\n",
    "\n",
    "for s, q in zip(ss, qq):\n",
    "    title, rel = get_wiki_entry(s, q)\n",
    "    print(\"Q: %s, title %s\" % (s, title))\n",
    "    # print(rel)\n",
    "    print(\"\\n\")\n",
    "\n",
    "print(remove_duplicate_result('Lý Bí Lý Bí'))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3.8.15 ('pt_py3.8')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "7b5c5cac8dc8d37482e4a7f044d901a86d8bd0dbdfe4e132d452e0bc3fa49c32"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
